# -- CloudZero host to send metrics to.
host: api.cloudzero.com
# -- Account ID of the account the cluster is running in. This must be a string - even if it is a number in your system.
cloudAccountId: null
# -- Name of the clusters.
clusterName: null
# -- Region the cluster is running in.
region: null

# -- CloudZero API key. Required if existingSecretName is null.
apiKey: null
# -- If set, the agent will use the API key in this Secret to authenticate with CloudZero.
existingSecretName: null

# -- The following lists of metrics are required for CloudZero to function.
# -- Modifications made to these lists may cause issues with the processing of cluster data
kubeMetrics:
  - kube_node_info
  - kube_node_status_capacity
  - kube_pod_container_resource_limits
  - kube_pod_container_resource_requests
  - kube_pod_labels
  - kube_pod_info
containerMetrics:
  - container_cpu_usage_seconds_total
  - container_memory_working_set_bytes
  - container_network_receive_bytes_total
  - container_network_transmit_bytes_total
insightsMetrics:
  - go_memstats_alloc_bytes
  - go_memstats_heap_alloc_bytes
  - go_memstats_heap_idle_bytes
  - go_memstats_heap_inuse_bytes
  - go_memstats_heap_objects
  - go_memstats_last_gc_time_seconds
  - go_memstats_alloc_bytes
  - go_memstats_stack_inuse_bytes
  - go_goroutines
  - process_cpu_seconds_total
  - process_max_fds
  - process_open_fds
  - process_resident_memory_bytes
  - process_start_time_seconds
  - process_virtual_memory_bytes
  - process_virtual_memory_max_bytes
  - remote_write_timeseries_total
  - remote_write_response_codes_total
  - remote_write_payload_size_bytes
  - remote_write_failures_total
  - remote_write_records_processed_total
  - remote_write_db_failures_total
  - http_requests_total
  - storage_write_failure_total
prometheusMetrics:
  - go_memstats_alloc_bytes
  - go_memstats_heap_alloc_bytes
  - go_memstats_heap_idle_bytes
  - go_memstats_heap_inuse_bytes
  - go_memstats_heap_objects
  - go_memstats_last_gc_time_seconds
  - go_memstats_alloc_bytes
  - go_memstats_stack_inuse_bytes
  - go_goroutines
  - process_cpu_seconds_total
  - process_max_fds
  - process_open_fds
  - process_resident_memory_bytes
  - process_start_time_seconds
  - process_virtual_memory_bytes
  - process_virtual_memory_max_bytes
  - prometheus_agent_corruptions_total
  - prometheus_api_remote_read_queries
  - prometheus_http_requests_total
  - prometheus_notifications_alertmanagers_discovered
  - prometheus_notifications_dropped_total
  - prometheus_remote_storage_bytes_total
  - prometheus_remote_storage_histograms_failed_total
  - prometheus_remote_storage_histograms_total
  - prometheus_remote_storage_metadata_bytes_total
  - prometheus_remote_storage_metadata_failed_total
  - prometheus_remote_storage_metadata_retried_total
  - prometheus_remote_storage_metadata_total
  - prometheus_remote_storage_samples_dropped_total
  - prometheus_remote_storage_samples_failed_total
  - prometheus_remote_storage_samples_in_total
  - prometheus_remote_storage_samples_total
  - prometheus_remote_storage_shard_capacity
  - prometheus_remote_storage_shards
  - prometheus_remote_storage_shards_desired
  - prometheus_remote_storage_shards_max
  - prometheus_remote_storage_shards_min
  - prometheus_sd_azure_cache_hit_total
  - prometheus_sd_azure_failures_total
  - prometheus_sd_discovered_targets
  - prometheus_sd_dns_lookup_failures_total
  - prometheus_sd_failed_configs
  - prometheus_sd_file_read_errors_total
  - prometheus_sd_file_scan_duration_seconds
  - prometheus_sd_file_watcher_errors_total
  - prometheus_sd_http_failures_total
  - prometheus_sd_kubernetes_events_total
  - prometheus_sd_kubernetes_http_request_duration_seconds
  - prometheus_sd_kubernetes_http_request_total
  - prometheus_sd_kubernetes_workqueue_depth
  - prometheus_sd_kubernetes_workqueue_items_total
  - prometheus_sd_kubernetes_workqueue_latency_seconds
  - prometheus_sd_kubernetes_workqueue_longest_running_processor_seconds
  - prometheus_sd_kubernetes_workqueue_unfinished_work_seconds
  - prometheus_sd_kubernetes_workqueue_work_duration_seconds
  - prometheus_sd_received_updates_total
  - prometheus_sd_updates_delayed_total
  - prometheus_sd_updates_total
  - prometheus_target_scrape_pool_reloads_failed_total
  - prometheus_target_scrape_pool_reloads_total
  - prometheus_target_scrape_pool_sync_total
  - prometheus_target_scrape_pools_failed_total
  - prometheus_target_scrape_pools_total
  - prometheus_target_sync_failed_total
  - prometheus_target_sync_length_seconds
aggregatorMetrics:
  - go_gc_duration_seconds
  - go_gc_duration_seconds_count
  - go_gc_duration_seconds_sum
  - go_gc_gogc_percent
  - go_gc_gomemlimit_bytes
  - go_goroutines
  - go_memstats_alloc_bytes
  - go_memstats_heap_alloc_bytes
  - go_memstats_heap_idle_bytes
  - go_memstats_heap_inuse_bytes
  - go_memstats_heap_objects
  - go_memstats_last_gc_time_seconds
  - go_memstats_alloc_bytes
  - go_memstats_stack_inuse_bytes
  - go_threads
  - http_request_duration_seconds_bucket
  - http_request_duration_seconds_count
  - http_request_duration_seconds_sum
  - http_requests_total
  - prometheus_remote_storage_exemplars_in_total
  - prometheus_remote_storage_histograms_in_total
  - prometheus_remote_storage_samples_in_total
  - prometheus_remote_storage_string_interner_zero_reference_releases_total
  - promhttp_metric_handler_requests_in_flight
  - promhttp_metric_handler_requests_total
# -- Any items added to this array will be added to the metrics that are sent to CloudZero, in addition to the minimal labels that CloudZero requires.
additionalMetricLabels: []

# metricFilters is used to determine which metrics are sent to CloudZero, as
# well as whether they are considered to be cost metrics or observability
# metrics.
#
# There are two sets of filters for each type (cost/observability): name and
# labels. The name filters are applied to the name to determine whether the
# metric should be included in the relevant output. If it is to be included, the
# relevant labels filters are applied to each label to determine whether the
# label should be included.
#
# In the event that there are no filters, the subject is always assumed to
# match.
#
# Note that for each match type (exact, prefix, suffix, contains, regex) there
# is an "additional..." property. This is to allow you to supply supplemental
# filters without clobbering the defaults. In general, the "additional..."
# properties should be used in your overrides file, and the unprefixed versions
# should be left alone.
metricFilters:
  cost:
    name:
      exact:
        - container_cpu_usage_seconds_total
        - container_memory_working_set_bytes
        - container_network_receive_bytes_total
        - container_network_transmit_bytes_total
        - kube_node_info
        - kube_node_status_capacity
        - kube_pod_container_resource_limits
        - kube_pod_container_resource_requests
        - kube_pod_labels
        - kube_pod_info
      prefix:
        - "cloudzero_"
      suffix: []
      contains: []
      regex: []

      additionalExact: []
      additionalPrefix: []
      additionalSuffix: []
      additionalContains: []
      additionalRegex: []
    labels:
      exact:
        - board_asset_tag
        - container
        - created_by_kind
        - created_by_name
        - image
        - instance
        - name
        - namespace
        - node
      prefix:
        - "_"
        - "label_"
        - "app.kubernetes.io/"
        - "k8s."
      suffix: []
      contains: []
      regex: []
      additionalExact: []
      additionalPrefix: []
      additionalSuffix: []
      additionalContains: []
      additionalRegex: []

  observability:
    name:
      exact:
        - go_gc_duration_seconds
        - go_gc_duration_seconds_count
        - go_gc_duration_seconds_sum
        - go_gc_gogc_percent
        - go_gc_gomemlimit_bytes
        - go_goroutines
        - go_memstats_alloc_bytes
        - go_memstats_heap_alloc_bytes
        - go_memstats_heap_idle_bytes
        - go_memstats_heap_inuse_bytes
        - go_memstats_heap_objects
        - go_memstats_last_gc_time_seconds
        - go_memstats_stack_inuse_bytes
        - go_threads
        - http_request_duration_seconds_bucket
        - http_request_duration_seconds_count
        - http_request_duration_seconds_sum
        - http_requests_total
        - process_cpu_seconds_total
        - process_max_fds
        - process_open_fds
        - process_resident_memory_bytes
        - process_start_time_seconds
        - process_virtual_memory_bytes
        - process_virtual_memory_max_bytes
        - prometheus_agent_corruptions_total
        - prometheus_api_remote_read_queries
        - prometheus_http_requests_total
        - prometheus_notifications_alertmanagers_discovered
        - prometheus_notifications_dropped_total
        - prometheus_remote_storage_bytes_total
        - prometheus_remote_storage_exemplars_in_total
        - prometheus_remote_storage_histograms_failed_total
        - prometheus_remote_storage_histograms_in_total
        - prometheus_remote_storage_histograms_total
        - prometheus_remote_storage_metadata_bytes_total
        - prometheus_remote_storage_metadata_failed_total
        - prometheus_remote_storage_metadata_retried_total
        - prometheus_remote_storage_metadata_total
        - prometheus_remote_storage_samples_dropped_total
        - prometheus_remote_storage_samples_failed_total
        - prometheus_remote_storage_samples_in_total
        - prometheus_remote_storage_samples_total
        - prometheus_remote_storage_shard_capacity
        - prometheus_remote_storage_shards
        - prometheus_remote_storage_shards_desired
        - prometheus_remote_storage_shards_max
        - prometheus_remote_storage_shards_min
        - prometheus_remote_storage_string_interner_zero_reference_releases_total
        - prometheus_sd_azure_cache_hit_total
        - prometheus_sd_azure_failures_total
        - prometheus_sd_discovered_targets
        - prometheus_sd_dns_lookup_failures_total
        - prometheus_sd_failed_configs
        - prometheus_sd_file_read_errors_total
        - prometheus_sd_file_scan_duration_seconds
        - prometheus_sd_file_watcher_errors_total
        - prometheus_sd_http_failures_total
        - prometheus_sd_kubernetes_events_total
        - prometheus_sd_kubernetes_http_request_duration_seconds
        - prometheus_sd_kubernetes_http_request_total
        - prometheus_sd_kubernetes_workqueue_depth
        - prometheus_sd_kubernetes_workqueue_items_total
        - prometheus_sd_kubernetes_workqueue_latency_seconds
        - prometheus_sd_kubernetes_workqueue_longest_running_processor_seconds
        - prometheus_sd_kubernetes_workqueue_unfinished_work_seconds
        - prometheus_sd_kubernetes_workqueue_work_duration_seconds
        - prometheus_sd_received_updates_total
        - prometheus_sd_updates_delayed_total
        - prometheus_sd_updates_total
        - prometheus_target_scrape_pool_reloads_failed_total
        - prometheus_target_scrape_pool_reloads_total
        - prometheus_target_scrape_pool_sync_total
        - prometheus_target_scrape_pools_failed_total
        - prometheus_target_scrape_pools_total
        - prometheus_target_sync_failed_total
        - prometheus_target_sync_length_seconds
        - promhttp_metric_handler_requests_in_flight
        - promhttp_metric_handler_requests_total
        - remote_write_db_failures_total
        - remote_write_failures_total
        - remote_write_payload_size_bytes
        - remote_write_records_processed_total
        - remote_write_response_codes_total
        - remote_write_timeseries_total
        - storage_write_failure_total
      prefix: []
      suffix: []
      contains: []
      regex: []
      additionalExact: []
      additionalPrefix: []
      additionalSuffix: []
      additionalContains: []
      additionalRegex: []
    labels:
      exact: []
      prefix: []
      suffix: []
      contains: []
      regex: []
      additionalExact: []
      additionalPrefix: []
      additionalSuffix: []
      additionalContains: []
      additionalRegex: []

prometheusConfig:
  configMapNameOverride: ''
  configMapAnnotations: {}
  configOverride: ''
  globalScrapeInterval: 120s
  scrapeJobs:
    # -- Enables the kube-state-metrics scrape job.
    kubeStateMetrics:
      enabled: true
      scrapeInterval: 120s  # Scrape interval for kubeStateMetrics job
    # -- Enables the cadvisor scrape job.
    cadvisor:
      enabled: true
      scrapeInterval: 120s  # Scrape interval for nodesCadvisor job
    # -- Enables the prometheus scrape job.
    prometheus:
      enabled: true
      scrapeInterval: 120s  # Scrape interval for prometheus job
    aggregator:
      enabled: true
      scrapeInterval: 120s  # Scrape interval for aggregator job
    # -- Any items added to this list will be added to the Prometheus scrape configuration.
    additionalScrapeJobs: []

# General server settings that apply to both the prometheus agent server and the webhook server
serverConfig:
  # -- The agent will use this file path on the container filesystem to get the CZ API key.
  containerSecretFilePath: /etc/config/secrets/
  # -- The agent will look for a file with this name to get the CZ API key.
  containerSecretFileName: value

# -- The following settings are for the init-backfill-job, which is used to backfill data from the cluster to CloudZero.
initBackfillJob:
  # -- By default, all image settings use those set in insightsController.server. Optionally use the below to override. This should not be common.
  # imagePullSecrets: []
  # image:
  #   repository: ghcr.io/cloudzero/cloudzero-insights-controller/cloudzero-insights-controller
  #   tag: 0.1.1
  #   pullPolicy: Always
  enabled: true

# -- This is a deprecated field that is replaced by initBackfillJob. However, the fields are identical, and initScrapeJob can still be used to configure the backFill/scrape Job.
# initScrapeJob:
  # -- By default, all image settings use those set in insightsController.server. Optionally use the below to override. This should not be common.
  # imagePullSecrets: []
  # image:
  #   repository: ghcr.io/cloudzero/cloudzero-insights-controller/cloudzero-insights-controller
  #   tag: 0.1.1
  #   pullPolicy: Always


initCertJob:
  enabled: true
  # -- Defaults to the same setting as the insightsController.server if set, otherwise left empty.
  # imagePullSecrets: []
  image:
    repository: bitnami/kubectl
    pullPolicy: Always
    tag: "1.32.0"

kubeStateMetrics:
  enabled: true
  image:
    registry: registry.k8s.io
    repository: kube-state-metrics/kube-state-metrics
    tag: "v2.10.1"
  nameOverride: "cloudzero-state-metrics"
  # Disable CloudZero KSM as a Scrape Target since the service endpoint is explicitly defined
  # by the Validators config file.
  prometheusScrape: false
  # Set a default port other than 8080 to avoid collisions with any existing KSM services.
  service:
    port: 8080

  # -- Overriding static scrape target address for an existing KSM.
  # -- Set to service <service-name>.<namespace>.svc.cluster.local:port if built-in is disabled (enable=false above)
  # targetOverride: kube-state-metrics.monitors.svc.cluster.local:8080
  # -- If targetOverride is set and kubeStateMetrics.enabled is true, it is likely that fullnameOverride below must be set as well.
  # -- This should not be a common configuration
  # fullnameOverride: "kube-state-metrics"

# -- Annotations to be added to the Secret, if the chart is configured to create one
secretAnnotations: {}
imagePullSecrets: []

scheme: https
endpoint: /v1/container-metrics

# environment validator image allows for CI to use a different image in testing
validator:
  serviceEndpoints:
    kubeStateMetrics:
  # -- Flag to skip validator failure if unable to connect to the CloudZero API.
  name: env-validator
  image:
    repository: ghcr.io/cloudzero/cloudzero-agent-validator/cloudzero-agent-validator
    tag: 0.10.0
    digest:
    pullPolicy: Always

server:
  name: server
  image:
    repository: quay.io/prometheus/prometheus
    # if not set appVersion field from Chart.yaml is used
    tag: ""
    # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).
    digest: ""
    pullPolicy: IfNotPresent
  nodeSelector: {}
  resources:
    requests:
      memory: 512Mi
      cpu: 250m
    limits:
      memory: 1024Mi
  deploymentAnnotations: {}
  podAnnotations: {}
  agentMode: true
  args:
  - --config.file=/etc/config/prometheus/configmaps/prometheus.yml
  - --web.enable-lifecycle
  - --web.console.libraries=/etc/prometheus/console_libraries
  - --web.console.templates=/etc/prometheus/consoles
  persistentVolume:
    existingClaim: ""
    enabled: false
    mountPath: /data
    subPath: ""
    storageClass: ""
    size: 8Gi
    accessModes:
    - ReadWriteOnce
  # --Limit the size to 8Gi to lower impact on the cluster, and to provide a reasonable backup for the WAL
  emptyDir:
    sizeLimit: 8Gi

insightsController:
  enabled: true
  labels:
    enabled: true
    patterns:
      - 'app.kubernetes.io/component'
      # - '.*'
    resources:
      pods: true
      namespaces: true
      deployments: false
      statefulsets: false
      nodes: false
      jobs: false
      cronjobs: false
      daemonsets: false
  annotations:
    enabled: false
    patterns:
      - '.*'
    resources:
      pods: true
      namespaces: true
      deployments: false
      statefulsets: false
      nodes: false
      jobs: false
      cronjobs: false
      daemonsets: false
  tls:
    # -- If disabled, the insights controller will not mount a TLS certificate from a Secret, and the user is responsible for configuring a method of providing TLS information to the webhook-server container.
    enabled: true
    # -- If left as an empty string, the certificate will be generated by the chart. Otherwise, the provided value will be used.
    crt: ""
    # -- If left as an empty string, the certificate private key will be generated by the chart. Otherwise, the provided value will be used.
    key: ""
    secret:
      # -- If set to true, a Secret will be created to store the TLS certificate and key.
      create: true
      # -- If set, the Secret will be created with this name. Otherwise, a default name will be generated.
      name: ""
    # -- The following TLS certificate information is for a self signed certificate. It is used as a default value for the validating admission webhook and the webhook server.
    # -- This path determines the location within the container where the TLS certificate and key will be mounted.
    mountPath: /etc/certs
    # -- This is the caBundle used by the Validating Admission Webhook when sending requests to the webhook server. If left empty, the default self-signed certificate will be used.
    # Set this value to an empty string if using cert-manager to manage the certificate instead. Otherwise, set this to the base64 encoded caBundle of the desired certificate.
    caBundle: ""
    # -- If enabled, the certificate will be managed by cert-manager, which must already be present in the cluster.
    # If disabled, a default self-signed certificate will be used.
    useCertManager: false
  server:
    name: webhook-server
    replicaCount: 3
    # -- Uncomment to use a specific imagePullSecrets; otherwise, the default top level imagePullSecrets is used.
    # imagePullSecrets: []
    image:
      repository: ghcr.io/cloudzero/cloudzero-insights-controller/cloudzero-insights-controller
      tag: 0.1.1
      pullPolicy: Always
    port: 8443
    read_timeout: 10s
    write_timeout: 10s
    send_timeout: 1m
    send_interval: 1m
    idle_timeout: 120s
    logging:
      level: info
    healthCheck:
      enabled: true
      path: /healthz
      port: 8443
      initialDelaySeconds: 15
      periodSeconds: 20
      timeoutSeconds: 3
      successThreshold: 1
      failureThreshold: 5
  volumeMounts: []
  volumes: []
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
  podLabels: {}
  service:
    port: 443
  webhooks:
    annotations: {}
    namespaceSelector: {}
    configurations:
      pods:
        path: /validate/pod
        apiGroups: ['""']
      namespaces:
        path: /validate/namespace
        apiGroups: ['""']
      deployments:
        path: /validate/deployment
        apiGroups: ["apps"]
      statefulsets:
        path: /validate/statefulset
        apiGroups: ["apps"]
      nodes:
        path: /validate/node
        apiGroups: ['""']
      jobs:
        path: /validate/job
        apiGroups: ["batch"]
      cronjobs:
        path: /validate/cronjob
        apiGroups: ["batch"]
      daemonsets:
        path: /validate/daemonset
        apiGroups: ["apps"]

serviceAccount:
  create: true
  name: ""
  annotations: {}

rbac:
  create: true

commonMetaLabels: {}

configmapReload:
  reloadUrl: ""
  env: []
  prometheus:
    enabled: true
    image:
      repository: quay.io/prometheus-operator/prometheus-config-reloader
      tag: v0.70.0
      digest: ""
      pullPolicy: IfNotPresent

    containerSecurityContext: {}
    resources: {}

# Aggregator application. This is the recommended way to deploy the Cloudzero insights controller.
#
# The aggregator architecture will deploy 2 more applications in conjunction with the insights admissions controller.
# The collector application will act as a remote write endpoint for prometheus to send the gathered metrics from the
# admissions controller, and write them to temporary files on disk.
# 
# The shipper application will process completed metrics files and push them to the remote server. This architecture
# will reduce upload error rates and data loss, along with decoupling the metric collection and shipment components
# offering greater flexibility for clusters with differing workloads.
aggregator:
  enabled: true
  replicas: 1
  logging:
    level: info
  # The location that the aggregator will mount a read-write directory. This is where intermediate metric files will exist.
  # Set `aggregator.database.purgeRules` to control the cleanup behavior of this directory
  mountRoot: /cloudzero
  image:
    repository: ghcr.io/cloudzero/cloudzero-insights-controller/cloudzero-insights-controller
    tag: latest
    pullPolicy: IfNotPresent
  cloudzero:
    # Interval between attempts to ship metrics to the remote endpoint.
    sendInterval: 1m
    # Max time the aggregator will spend attempting to ship metrics to the remote endpoint.
    sendTimeout: 30s
    rotateInterval: 30m
  database:
    # Max number of records per file. Use this to adjust file sizes uploaded to the server. The default value is good in most cases.
    maxRecords: 1500000
    # Max interval to flush a metrics file. This is mostly useful for smaller clusters with little activity.
    maxInterval: 10m
    # Level of compression on the files. This ranges from 0-9 with tradeoffs between speed and size.
    # Read more about brotli compression here: https://github.com/google/brotli/blob/master/c/tools/brotli.md#options
    compressionLevel: 8
    # purgeRules defines the rules to follow when removing metric files that have already been uploaded to the Cloudzero platform
    purgeRules:
      # How long to keep uploaded files. This option can be useful to optimize the storage required by the collector/shipper architecture on your nodes.
      # `2160h` is 90 days, and is a reasonable default. This can reasonably be any value, as the application will force remove files if space is constrained.
      # `0s` is also a valid option and can signify that you do not want to keep uploaded files at all. Though do note, that this could possibly
      # result in data loss if there are transient upload failures during the lifecycle of the application.
      metricsOlderThan: 2160h
      # If set to true (default), then files older than `metricsOlderThan` will not be deleted unless there is detected storage pressure.
      # For example, if there are files older than `metricsOlderThan` but only 30% of storage space is used, the files will not be deleted.
      lazy: true
      # This controls the percentage of files the application will remove when there is critical storage pressure. This is defined by >95% of storage usage.
      percent: 20
  collector:
    port: 8080
    resources:
      requests:
        memory: "64Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
  shipper:
    resources:
      requests:
        memory: "64Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
