# -- CloudZero host to send metrics to.
host: api.cloudzero.com
# -- Account ID of the account the cluster is running in. This must be a string - even if it is a number in your system.
cloudAccountId: null
# -- Name of the clusters.
clusterName: null
# -- Region the cluster is running in.
region: null

# -- CloudZero API key. Required if existingSecretName is null.
apiKey: null
# -- If set, the agent will use the API key in this Secret to authenticate with CloudZero.
existingSecretName: null

# Agent largely contains top-level settings which are often shared by multiple
# components within this chart, or used as defaults in case values are not
# explicitly set per-component.
defaults:
  # The default image settings which will be fallen back on for all components.
  #
  # Note that all image values (including repository, tag, etc.) are valid here,
  # though for the most part they are overridden by the component-specific
  # settings anyways.
  image:
    pullPolicy: IfNotPresent
    pullSecrets:
  # If set, these DNS settings will be attached to resources which support it.
  dns:
    # DNS policy to use on all pods.
    #
    # Valid values include:
    #
    # - "Default"
    # - "ClusterFirst"
    # - "ClusterFirstWithHostNet"
    # - "None"
    #
    # Somewhat counterintuitively, "Default" is not actually the default,
    # "ClusterFirst" is.
    #
    # For details, see the Kubernetes documentation:
    # https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
    #
    # Note that if you set this, you'll likely also want to set kubeStateMetrics.dnsPolicy
    # to the same value.
    policy:
    # DNS configuration to use on all pods.
    #
    # There are currently three properties which can be specified: nameservers,
    # searches, and options.
    #
    # For details, see the Kubernetes documentation:
    # https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config
    #
    # Note that if you set this, you'll likely also want to set kubeStateMetrics.dnsConfig
    # to the same value.
    config: {}
  # Labels to be added to all resources.
  #
  # Labels are organized as key/value pairs. For example, if you wanted to set a
  # my.org/team label to the value "superstars":
  #
  #   labels:
  #     my.org/team: superstars
  #
  # Note that this chart will unconditionally add the following labels:
  #
  #  - app.kubernetes.io/version
  #  - helm.sh/chart
  #  - app.kubernetes.io/managed-by
  #  - app.kubernetes.io/part-of
  #
  # Additionally, certain components will add additional labels. Any labels
  # specified here will be *in addition* to the labels added automatically, not
  # instead of them.
  #
  # For more information, see the Kubernetes documentation:
  # https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  #
  # You may also be interested in this list of well-known labels:
  # https://kubernetes.io/docs/reference/labels-annotations-taints/
  #
  # Note that if you set this, you'll likely also want to set
  # kubeStateMetrics.customLabels.
  labels: {}
  # Annotations to be added to all resources.
  #
  # Similar to labels, annotations are organized as key/value pairs, and
  # annotations specified here will be merged into any annotations added
  # automatically by the chart.
  #
  # For more information, see the Kubernetes documentation:
  # https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  #
  # Note that if you set this, you'll likely also want to set
  # kubeStateMetrics.annotations.
  annotations: {}
  # Affinity settings to be added to all resources.
  #
  # Affinity settings are used to control the scheduling of pods. For more
  # information, see the Kubernetes documentation:
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  #
  # Note that if you set this, you'll likely also want to set
  # kubeStateMetrics.affinity.
  affinity: {}
  # Tolerations to be added to all resources.
  #
  # Tolerations are used to control the scheduling of pods. For more
  # information, see the Kubernetes documentation:
  # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  #
  # Note that if you set this, you'll likely also want to set
  # kubeStateMetrics.tolerations.
  tolerations: []
  # Node Selector to be added to all resources.
  #
  # Node Selector is used to control the scheduling of pods. For more
  # information, see the Kubernetes documentation:
  # https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  #
  # Note that if you set this, you'll likely also want to set
  # kubeStateMetrics.nodeSelector.
  nodeSelector: {}
  # If set, this priority class name will be used for all deployments and jobs.
  #
  # Note that, if used, you will need to create the PriorityClass resource
  # yourself; this chart is only capable of referencing an existing priority
  # class, not creating one from whole cloth.
  #
  # For more information, see the Kubernetes documentation:
  # https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/
  #
  # Note that if you set this, you'll likely also want to set
  # kubeStateMetrics.priorityClassName.
  priorityClassName:

# Component-specific configuration settings.
components:
  # The agent here refers to the CloudZero Agent, which contains most of the
  # code that makes this chart work. Since 1.1, CloudZero uses a single
  # container image, with multiple executables, to provide CloudZero
  # functionality.
  agent:
    image:
      repository: ghcr.io/cloudzero/cloudzero-agent/cloudzero-agent
      tag: 1.1.0-rc-1  # <- Software release corresponding to this chart version.
  # kubectl contains details about where to find the kubectl image.  This chart
  # uses the kubectl image as part of the job to initialize certificates.
  kubectl:
    image:
      repository: docker.io/bitnami/kubectl
      tag: "1.32.0"
  # prometheus contains details about where to find the Prometheus image.
  # Prometheus is critical to the functionality of this chart, and is used to
  # scrape metrics.
  prometheus:
    image:
      repository: quay.io/prometheus/prometheus
      tag:  # This will fall back on .Chart.AppVersion if not set.
  # prometheusReloader contains details about where to find the Prometheus
  # reloader image.
  #
  # prometheus-config-reloader will watch the Prometheus configuration for
  # changes and restart the Prometheus pod as necessary.
  prometheusReloader:
    image:
      repository: quay.io/prometheus-operator/prometheus-config-reloader
      tag: "v0.70.0"

# Due to limitations of Helm, we are unfortunately not able to automatically configure the
# kube-state-metrics subchart using the configuration above, and instead need to configure
# it here, often duplicating things.
#
# For full documentation on configuring this subchart, see:
# https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics
# Specifically, the values.yaml file in that repository:
# https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-state-metrics/values.yaml
kubeStateMetrics:
  enabled: true
  affinity: {}
  tolerations: []
  customLabels: {}
  dnsPolicy: ClusterFirst
  dnsConfig: {}
  annotations: {}
  podAnnotations: {}
  nodeSelector: {}
  podDisruptionBudget: {}
  image:
    registry: registry.k8s.io
    repository: kube-state-metrics/kube-state-metrics
    tag: "v2.10.1"
    sha:
    pullPolicy: IfNotPresent
  imagePullSecrets: []
  nameOverride: "cloudzero-state-metrics"
  # Disable CloudZero KSM as a Scrape Target since the service endpoint is
  # explicitly defined by the Validators config file.
  prometheusScrape: false
  # Set a default port other than 8080 to avoid collisions with any existing KSM
  # services.
  service:
    port: 8080

#######################################################################################
#######################################################################################
####                                                                               ####
####  Values below this point are not considered API stable. Use at your own risk. ####
####  If you do require them for some reason, please let us know so we can work on ####
####  covering your use case in the stable section.                                ####
####                                                                               ####
#######################################################################################
#######################################################################################

prometheusConfig:
  configMapNameOverride: ""
  configMapAnnotations: {}
  configOverride: ""
  globalScrapeInterval: 60s
  scrapeJobs:
    # -- Enables the kube-state-metrics scrape job.
    kubeStateMetrics:
      enabled: true
      # Scrape interval for kubeStateMetrics job
      scrapeInterval: 60s
    # -- Enables the cadvisor scrape job.
    cadvisor:
      enabled: true
      # Scrape interval for nodesCadvisor job
      scrapeInterval: 60s
    # -- Enables the prometheus scrape job.
    prometheus:
      enabled: true
      # Scrape interval for prometheus job
      scrapeInterval: 120s
    aggregator:
      enabled: true
      # Scrape interval for aggregator job
      scrapeInterval: 120s
    # -- Any items added to this list will be added to the Prometheus scrape configuration.
    additionalScrapeJobs: []

# General server settings that apply to both the prometheus agent server and the webhook server
serverConfig:
  # -- The agent will use this file path on the container filesystem to get the CZ API key.
  containerSecretFilePath: /etc/config/secrets/
  # -- The agent will look for a file with this name to get the CZ API key.
  containerSecretFileName: value

# -- The following settings are for the init-backfill-job, which is used to backfill data from the cluster to CloudZero.
initBackfillJob:
  annotations: {}
  tolerations: []
  # -- By default, all image settings use those set in insightsController.server. Optionally use the below to override. This should not be common.
  # imagePullSecrets: []
  image:
    repository:
    tag:
    digest:
    pullPolicy:
  enabled: true

# -- This is a deprecated field that is replaced by initBackfillJob. However, the fields are identical, and initScrapeJob can still be used to configure the backFill/scrape Job.
# initScrapeJob:
# -- By default, all image settings use those set in insightsController.server. Optionally use the below to override. This should not be common.
# imagePullSecrets: []
# image:
#   repository:
#   tag:
#   pullPolicy:

initCertJob:
  enabled: true
  # -- Defaults to the same setting as the insightsController.server if set, otherwise left empty.
  # imagePullSecrets: []
  annotations: {}
  tolerations: []
  image:
    repository:
    pullPolicy:
    digest:
    tag:
  rbac:
    create: true
    serviceAccountName: ""
    clusterRoleName: ""
    clusterRoleBindingName: ""

  # -- Overriding static scrape target address for an existing KSM.
  # -- Set to service <service-name>.<namespace>.svc.cluster.local:port if built-in is disabled (enable=false above)
  # targetOverride: kube-state-metrics.monitors.svc.cluster.local:8080
  # -- If targetOverride is set and kubeStateMetrics.enabled is true, it is likely that fullnameOverride below must be set as well.
  # -- This should not be a common configuration
  # fullnameOverride: "kube-state-metrics"

# -- Annotations to be added to the Secret, if the chart is configured to create one
secretAnnotations: {}
imagePullSecrets: []

scheme: https
endpoint: /v1/container-metrics

# environment validator image allows for CI to use a different image in testing
validator:
  serviceEndpoints:
    kubeStateMetrics:
  # -- Flag to skip validator failure if unable to connect to the CloudZero API.
  name: env-validator
  image:
    repository:
    tag:
    digest:
    pullPolicy:
    pullSecrets:

server:
  name: server
  image:
    repository:
    tag:
    digest:
    pullPolicy:
  nodeSelector: {}
  resources:
    requests:
      memory: 512Mi
      cpu: 250m
    limits:
      memory: 1024Mi
  deploymentAnnotations: {}
  podAnnotations: {}
  agentMode: true
  args:
    - --config.file=/etc/config/prometheus/configmaps/prometheus.yml
    - --web.enable-lifecycle
    - --web.console.libraries=/etc/prometheus/console_libraries
    - --web.console.templates=/etc/prometheus/consoles
  persistentVolume:
    existingClaim: ""
    enabled: false
    mountPath: /data
    subPath: ""
    storageClass: ""
    size: 8Gi
    accessModes:
      - ReadWriteOnce
    annotations: {}
  # --Limit the size to 8Gi to lower impact on the cluster, and to provide a reasonable backup for the WAL
  emptyDir:
    sizeLimit: 8Gi
  affinity: {}
  tolerations: []

insightsController:
  enabled: true
  labels:
    enabled: true
    patterns:
      - "app.kubernetes.io/component"
      # - '.*'
  annotations:
    enabled: false
    patterns:
      - ".*"
  tls:
    # -- If disabled, the insights controller will not mount a TLS certificate from a Secret, and the user is responsible for configuring a method of providing TLS information to the webhook-server container.
    enabled: true
    # -- If left as an empty string, the certificate will be generated by the chart. Otherwise, the provided value will be used.
    crt: ""
    # -- If left as an empty string, the certificate private key will be generated by the chart. Otherwise, the provided value will be used.
    key: ""
    secret:
      # -- If set to true, a Secret will be created to store the TLS certificate and key.
      create: true
      # -- If set, the Secret will be created with this name. Otherwise, a default name will be generated.
      name: ""
    # -- The following TLS certificate information is for a self signed certificate. It is used as a default value for the validating admission webhook and the webhook server.
    # -- This path determines the location within the container where the TLS certificate and key will be mounted.
    mountPath: /etc/certs
    # -- This is the caBundle used by the Validating Admission Webhook when sending requests to the webhook server. If left empty, the default self-signed certificate will be used.
    # Set this value to an empty string if using cert-manager to manage the certificate instead. Otherwise, set this to the base64 encoded caBundle of the desired certificate.
    caBundle: ""
    # -- If enabled, the certificate will be managed by cert-manager, which must already be present in the cluster.
    # If disabled, a default self-signed certificate will be used.
    useCertManager: false
  server:
    name: webhook-server
    replicaCount: 3
    # -- Uncomment to use a specific imagePullSecrets; otherwise, the default top level imagePullSecrets is used.
    # imagePullSecrets: []
    image:
      repository:
      tag:
      pullPolicy:
    port: 8443
    read_timeout: 10s
    write_timeout: 10s
    send_timeout: 1m
    send_interval: 1m
    idle_timeout: 120s
    logging:
      level: info
    healthCheck:
      enabled: true
      path: /healthz
      port: 8443
      initialDelaySeconds: 15
      periodSeconds: 20
      timeoutSeconds: 3
      successThreshold: 1
      failureThreshold: 5
    nodeSelector: {}
    tolerations: []
    affinity: {}
    deploymentAnnotations: {}
    podAnnotations: {}
  volumeMounts: []
  volumes: []
  resources: {}
  podAnnotations: {}
  podLabels: {}
  service:
    port: 443
  webhooks:
    annotations: {}
    namespaceSelector: {}
    path: /validate

serviceAccount:
  create: true
  name: ""
  annotations: {}

rbac:
  create: true

commonMetaLabels: {}

configmapReload:
  reloadUrl: ""
  env: []
  prometheus:
    enabled: true
    image:
      repository:
      tag:
      digest:
      pullPolicy:

    containerSecurityContext: {}
    resources: {}

# The aggregator provides an intermediary between the CloudZero Agent and the CloudZero API.
# It is composed of two applications, the collector and the shipper.

# The collector application provides an endpoint for the CloudZero Agent to write metrics to.
# It filters out any unwanted metrics as it receives them, aggregates the wanted metrics,
# and stores them in a compressed format on disk until they are ready to be uploaded to the
# CloudZero servers. Once the collector has aggregated sufficient metrics (or a given amount of time has elapsed)
# the data is sent to the shipper.

# The shipper will process the completed metrics files and push them to the remote server.
# It will also handle any requests from the server to re-send any missing or incomplete data,
# ensuring that there is no data loss in the event of any loss of communication with the CloudZero API,
# even when a misconfiguration (such as an incorrect API key) prevents it.
aggregator:
  replicas: 1
  logging:
    # Logging level that will be posted to stdout.
    # Valid values are: 'debug', 'info', 'warn', 'error'
    level: info
  # Top-level directory containing CloudZero data. There will be subdirectories for configuration (the mounted ConfigMap)
  # and the API key (typically a mounted Secret), and data to be uploaded to CloudZero, specifically metrics.
  # This value is really only visible internally in the container, so you shouldn't generally need to change it.
  # Set `aggregator.database.purgeRules` to control the cleanup behavior of this directory
  mountRoot: /cloudzero
  # Whether to enable the profiling endpoint (/debug/pprof/). This should
  # generally be disabled in production.
  profiling: false
  image:
    repository:
    tag:
    digest:
    pullPolicy:
  cloudzero:
    # Interval between attempts to ship metrics to the remote endpoint.
    sendInterval: 1m
    # Max time the aggregator will spend attempting to ship metrics to the remote endpoint.
    sendTimeout: 30s
    rotateInterval: 30m
  database:
    # Max number of records per file. Use this to adjust file sizes uploaded to the server. The default value is good in most cases.
    maxRecords: 1500000
    # Max interval to flush a metrics file. This is mostly useful for smaller clusters with little activity.
    maxInterval: 10m
    # Compression level to use when compressing metrics files on-disk.
    # Valid value range from 0-11, with higher values yielding improved compression ratios
    # at the expense of speed and memory usage.
    # Read more about brotli compression here: https://github.com/google/brotli/blob/master/c/tools/brotli.md#options
    compressionLevel: 8
    # The rules that the application will follow in respect to cleaning up old files that have been uploaded to the
    # Cloudzero platform.
    # Generally, the defaults will be okay for the majority of use cases. But, the options are here for more advanced
    # users to optimize disk usage. For example, the default case is to keep uploaded files around for 90 days, as this
    # falls in line with most customer's data tolerance policies. But, if deployed on a more active and/or larger cluster,
    # this value can be lowered to keep disk usage lower with the tradeoff of less data-retention. Regardless of what you
    # define here if there is disk pressure detected, files will be deleted (oldest first) to free space.
    purgeRules:
      # How long to keep uploaded files. This option can be useful to optimize the storage required by the collector/shipper
      # architecture on your nodes.
      # `2160h` is 90 days, and is a reasonable default. This can reasonably be any value, as the application will force
      # remove files if space is constrained.
      # `0s` is also a valid option and can signify that you do not want to keep uploaded files at all. Though do note
      # that this could possibly result in data loss if there are transient upload failures during the lifecycle of the application.
      metricsOlderThan: 2160h
      # If set to true (default), then files older than `metricsOlderThan` will not be deleted unless there is detected storage pressure.
      # For example, if there are files older than `metricsOlderThan` but only 30% of storage space is used, the files will not be deleted.
      lazy: true
      # This controls the percentage of files the application will remove when there is critical storage pressure.
      # This is defined by >95% of storage usage.
      percent: 20
    emptyDir:
      enabled: true
      sizeLimit: ""
  collector:
    port: 8080
    resources:
      requests:
        memory: "64Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
  shipper:
    port: 8081
    resources:
      requests:
        memory: "64Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
  nodeSelector: {}
  tolerations: []
  affinity: {}
